# 07.09.24

going over the GPT_from_scratch code that I implemented. 
- what is nn.Embedding and how is it different from nn.Linear layer?
- why do we pass the computation of most layers/blocks through a linearLayer?
- where do i know to apply dropout?
- nn.Sequential vs nn.ModuleList

# 07.10.24

implementing GPT tokenizer from scratch


# 07.12.24

- i dont understand cross entropy and loss claculation given a forward pass is done and we have the logits of shape (B, ctx_len, vocab_size)
- for dropout we the OG gpt applies it after calculating affinities inside AttnHead, after multiHeadAttn, and the feedforward layer. 
- we want to apply dropout to the parts of the network with the largest/densest param tensors as a general rule/intuition
- the masked_fill for wei should be applied with a fixed dimension for the self.tril[:T, :T] where T is the current ctx_len of the passed in batch of tensors since they can be less than the full ctx_len
(at least i think so)
- loss estimation needs to happen for both train and val data (duhhhhh!)

# 07.14.24

- keeping all else the same (on a toy file of taylorswift wiki article text as train data) it looks like when i have:
  - n_embed = 128 -> train_loss: 1.447 | val_loss: 1.799
  - n_embed = 384 -> train_loss: 0.829 | val_loss: 1.725
but i dont understand why that is happening? is the embedding vector being large make it underfit by a lot? why?

- starting GPT-2 karpathy video
  - need to better understand pytorch operations for effecient implementation
  - dont know much about the weight initialization and some of the weight typing(tok_embed and lm_head) aspects of the network

# 07.17.24
- F.cross_entropy() doesnt like multidimensional vectors, so we need to flatetn it out to 2D tensors
- for a random init network we want every token in the vocab to get roughly uniform probability so no one token is favored to much from the start. random init loss should be about -ln(1/vocab_size) for sanity checking
- weight sharing/tying the lm_head and the tok_embedding linear layers. why do we do it? need to clarify on this more
- weight initialization: pytorch already does weight initialization for us in the background(xavier or kaiming_uniform) but we can also manually assign it ourselves and that is what is done in GPT-2
  - for weights its a normal dist with a stddev of 0.02
  - for bias its a constant of 0 (this is not the pytorch default, its uniform init by default)
  - for token embeds its also a normal dist with stddev of 0.02
  - pos_embeds are normal dist with stddev of 0.01

# 07.18.24
- what does the sqrt(head_size) do in the attention computation step of `wei=(q @ k.transpose())/sqrt(head_size)`? like what kind of effect conceptually does this scaling have?

# 07.19.24
- Game plan for the day:
  [x] Implement weight init in gpt2
	- im not exactly understanding the std dev initialization scaling for the MLP and SelfAttention. I know conceptually its to not blow up the variance in x as residual streams add their weights on every block. but what does that really mean? 
  [x] Re-implement CausalSelfAttention again for practice as it was confusing me yesterday
	- messed up on the autoregressive mask(bias) by initializing to n_embd instead of block_size. it still worked though which is concerning. I see why its so easy to introduce subtle bugs into DL code now.
  [x] Implement DataloaderLite and make the basic training loop for gpt2
  [x] Implement the optimizations of TF32, mixed precision, flash attn, torch.compile, nice numbers 
  [x] Finish karpathy gpt2 video
	- didnt do the ddp portions but did add the configure_optimizers method to add weight decay to the layers involved in matmul
  [ ] try and get the BERT model working by making sure we can load the weights from the bert-base-uncased release and run inference on it

# 07.21.24
- game plan:
  [-] try and get the BERT model working by making sure we can load the weights from the bert-base-uncased release and run inference on it
	- struggling with how to create masks for the inputs so the attention calculation is faithful to BERT
	- i understand how to create the masks for MLM objective still not so clear on NSP, but one step at a time. chatGPT convo for this: https://chatgpt.com/c/c11ed3d9-bd70-40ba-aa50-2f99eb063882
	- got most of the model written and it mostly matches the hf impl. but have some weirdness with the attn_mask shape, weight tying, and other minor things that arent correct. working on it tomorrow. chatGPT convo for this: https://chatgpt.com/c/9f4ef798-b5c5-4206-b56a-f84d5b68fbcf

# 07.28.24
- havent focused on this for a bit. because i got frustrated trying to match the hf implementation to load inthe weights as is to make sure it works(in full transparency, i didnt try very hard and got frustrated instead)
- so we try again today, i think its how im loading the weights, we make a few different choices in tensor representations internally but all the shapes and sizes should be there just needs some manipulation to fit into my custom construct. 
- game plan:
  [ ] make a custom from_pretrained() class which maps the hf model weights to our model internal representation.
	- this involves jsut looking at the state_dict of hf_model and my sk_model and seeing which layers are different and how i can manipulate the weights to properly load into mine.

# 07.29.24
- Exploring some other model arches to see which one would be good to tackle next. The current choices are Qwen2, DeBerta, and MobileLLM.
- Qwen2:
  Tokenizer:
	- byte-level BPE tokenizer
	- vocab: 151,643 regular tokens + 3 special tokens
  Arch:
	- GQA(Grouped Query Attention) instead of MHA(Multi Headed Attention)
	- DCA(Dual Chunk Attention): segment long sequences into chunks of manageable size, positional info between tokens within and across chunks
	- YARN for rescaling attn weights for ctx_len extrapolation
	- SwiGLU for activation: not in pytorch and need to implement on my own
	- Rotary Position Embeddings for positional Embedding
	- QKV bias for attn
	- RMSNorm for pre-norm: it's in torch.nn.RMSNorm
- I'm going to implement Qwen2 as practice.
- The hard part will be trying to understand and implement dual chunk attention, but im thinking for first pass I can just do regular attention and later try to cover the case of input being longet than the ctx_len

# 07.30.24
- maybe llama is the thing to implement because it doesnt do Dual Chunk Attention but has the GQA, swiGLU, RoPE, and RMSNorm
- we are going with Llama-2 paper to implement.
- Llama-2:
  Tokenizer:
	- BPE tokenizer from SentencePiece: byte-level decompose fo unknown UTF-8 chars
	- all numbers get split into individual digits
	- vocab: 32k tokens
  Arch:
	- GQA(Grouped Query Attention)
	- pre-norm with RMSNorm
	- swiGLU as the activation function
	- RoPE(Rotary Positional Embeddings): only for Q, K vectors inside attention. (does this mean no pos encoding for V?)
	- KV cache
- in llama's MLP layer there is up, down, and gate proj Linear layers. dont have a good intuition on what these do.
  - looked it up and it makes a lot more sense now.
  - up_proj: takes the result coming out of attn_layer and projects it to a higher dimensional vector (therefore the 'up' term)
  - down_proj: takes that intermediate representation of the expanded high dimensional vector and projects it back down to the attn_layer hidden_dim size
  - gate_proj: also performs the same vector wideninig projection as the up_proj and gets multiplied with it as an additional layer. says for controlling the flow of info, which kinda makes sense since it gets multiplied into the up_proj result before continuing but not sure exactly how it changes them to 'gate' something
- starting the implementation to match the hf model so i can load in the 7B weights but as always not sure where to put the dropout layers. will look that up later.
- got the general model down, with MLP(w/ SwiGLU) and Block layer done.
- working with the GQA module. understanding how to reshape tensors to make the different n_head and kv_head sizes to compute together. I think i have the hang of it. but more to do tomorrow.
  
# 07.31.24
- continuing implementing llama-2 today. GQA is almost done, then moving on to rope embeddings inside of the attn mechanism, then KV cache
- before continuing im actually having to learn einops. gonna spend like 2 hours on it and then get back to the implementation.
- Einops:
  - Free Indices: idxs specified in the output
  - Summation Indicies: idxs that appear in the input but not in the output. any idxs like this will get summed over therefore the name `summation indicies`
  - Rules:
	- repeating leters in different inputs means they will be multiplied and the products will be the output. ex: `ik,kj->ij` (k is repeating so k across the inputs will get multiplied)
	- omitting a letter means that that axis will be summed. ex: `i->` (since i doesnt exist in the output that means we will sum the entire axis i and give that as output)
  - this shit is tough when you dont know linear algebra very well

# 08.08.24
- havent done this in a while. refocusing now
- to understand and slowly work through einops, going to try and replace the original GPT implementation with einsum based one to get the hang of it.
- Implemented it in gpt where the each attention head is separately calculated in its own class (so only 3 dimensions to deal with in einsum)
- Implemented it in gpt2 where we do multiHeadedAttention as one class using tensor operations for all the heads (so 4 dimensional einsum attention)
- shit is hard and confusing, should have payed attn in college. didnt get to run it GPT and GPT2 though, just crosschecked to see that wei.allclose(wei_einsum) was true in both instances. 

# 08.09.24
- implemented the attention operation without einops in llama2, now need to add rope to make it complete. KV cache and stuff we shall worry about later. just gotta be able to load and inference with pre-existing model weights they got.
- watched a video on rope embeddings implementation in llama2(https://www.youtube.com/watch?v=oM4VmoabDAI&t=4087s) and looking at impl in a git repo(https://github.com/aju22/LLaMA2/blob/main/model.py#L24)

# 08.10.24
- will replace the torch vector operations with einsum ops after I am able to load up the llama2 weights and perform inference on it.
- trying im implement rope into llama2 today
- RoPE notes:
  - cant apply rope to odd number head_dim vectors, as we block them by 2 each(pairs)
  -
