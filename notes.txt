# 07.09.24

going over the GPT_from_scratch code that I implemented. 
- what is nn.Embedding and how is it different from nn.Linear layer?
- why do we pass the computation of most layers/blocks through a linearLayer?
- where do i know to apply dropout?
- nn.Sequential vs nn.ModuleList

# 07.10.24

implementing GPT tokenizer from scratch


# 07.12.24

- i dont understand cross entropy and loss claculation given a forward pass is done and we have the logits of shape (B, ctx_len, vocab_size)
- for dropout we the OG gpt applies it after calculating affinities inside AttnHead, after multiHeadAttn, and the feedforward layer. 
- we want to apply dropout to the parts of the network with the largest/densest param tensors as a general rule/intuition
- the masked_fill for wei should be applied with a fixed dimension for the self.tril[:T, :T] where T is the current ctx_len of the passed in batch of tensors since they can be less than the full ctx_len
(at least i think so)
- loss estimation needs to happen for both train and val data (duhhhhh!)

# 07.14.24

- keeping all else the same (on a toy file of taylorswift wiki article text as train data) it looks like when i have:
  - n_embed = 128 -> train_loss: 1.447 | val_loss: 1.799
  - n_embed = 384 -> train_loss: 0.829 | val_loss: 1.725
but i dont understand why that is happening? is the embedding vector being large make it underfit by a lot? why?

- starting GPT-2 karpathy video
  - need to better understand pytorch operations for effecient implementation
  - dont know much about the weight initialization and some of the weight typing(tok_embed and lm_head) aspects of the network

# 07.17.24
- F.cross_entropy() doesnt like multidimensional vectors, so we need to flatetn it out to 2D tensors
- for a random init network we want every token in the vocab to get roughly uniform probability so no one token is favored to much from the start. random init loss should be about -ln(1/vocab_size) for sanity checking
- weight sharing/tying the lm_head and the tok_embedding linear layers. why do we do it? need to clarify on this more
- weight initialization: pytorch already does weight initialization for us in the background(xavier or kaiming_uniform) but we can also manually assign it ourselves and that is what is done in GPT-2
  - for weights its a normal dist with a stddev of 0.02
  - for bias its a constant of 0 (this is not the pytorch default, its uniform init by default)
  - for token embeds its also a normal dist with stddev of 0.02
  - pos_embeds are normal dist with stddev of 0.01

# 07.18.24
- what does the sqrt(head_size) do in the attention computation step of `wei=(q @ k.transpose())/sqrt(head_size)`? like what kind of effect conceptually does this scaling have?

# 07.19.24
- Game plan for the day:
  [x] Implement weight init in gpt2
	- im not exactly understanding the std dev initialization scaling for the MLP and SelfAttention. I know conceptually its to not blow up the variance in x as residual streams add their weights on every block. but what does that really mean? 
  [x] Re-implement CausalSelfAttention again for practice as it was confusing me yesterday
	- messed up on the autoregressive mask(bias) by initializing to n_embd instead of block_size. it still worked though which is concerning. I see why its so easy to introduce subtle bugs into DL code now.
  [x] Implement DataloaderLite and make the basic training loop for gpt2
  [x] Implement the optimizations of TF32, mixed precision, flash attn, torch.compile, nice numbers 
  [x] Finish karpathy gpt2 video
	- didnt do the ddp portions but did add the configure_optimizers method to add weight decay to the layers involved in matmul
  [ ] try and get the BERT model working by making sure we can load the weights from the bert-base-uncased release and run inference on it

# 07.21.24
- game plan:
  [-] try and get the BERT model working by making sure we can load the weights from the bert-base-uncased release and run inference on it
	- struggling with how to create masks for the inputs so the attention calculation is faithful to BERT
	- i understand how to create the masks for MLM objective still not so clear on NSP, but one step at a time. chatGPT convo for this: https://chatgpt.com/c/c11ed3d9-bd70-40ba-aa50-2f99eb063882
	- got most of the model written and it mostly matches the hf impl. but have some weirdness with the attn_mask shape, weight tying, and other minor things that arent correct. working on it tomorrow. chatGPT convo for this: https://chatgpt.com/c/9f4ef798-b5c5-4206-b56a-f84d5b68fbcf
