{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-04 12:15:44--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-07-04 12:15:44 (10.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data to train on (all works of shakesphere combined into one text file)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract all unique chars from the entire corpus\n",
    "all_chars = sorted(list(set(text)))\n",
    "print(\"\".join(all_chars))\n",
    "len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a tokenizer at char level\n",
    "\n",
    "stoi = {char:i for i, char in enumerate(all_chars)}\n",
    "itos = {i:char for i, char in enumerate(all_chars)}\n",
    "\n",
    "encode = lambda x: [stoi[char] for char in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you are a good boy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"you are a good boy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "encoded_text = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(encoded_text.shape, encoded_text.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540 1115394\n"
     ]
    }
   ],
   "source": [
    "# split corpus into train and validation sets\n",
    "\n",
    "split_n_idx = int(0.9 * len(text))\n",
    "train_data = encoded_text[:split_n_idx]\n",
    "val_data = encoded_text[split_n_idx:]\n",
    "\n",
    "print(len(train_data), len(val_data), len(train_data)+len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context length declared here\n",
    "# bc the transformer cant take the full sequence, it would bee too long and computationally infeasible for large corpus bc of the n^2 attention mechanism whee n is the num of token in the sequence\n",
    "# so we declare a smaller batch to be the input per training sample\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if input is tensor([18]) then output should be 47\n",
      "if input is tensor([18, 47]) then output should be 56\n",
      "if input is tensor([18, 47, 56]) then output should be 57\n",
      "if input is tensor([18, 47, 56, 57]) then output should be 58\n",
      "if input is tensor([18, 47, 56, 57, 58]) then output should be 1\n",
      "if input is tensor([18, 47, 56, 57, 58,  1]) then output should be 15\n",
      "if input is tensor([18, 47, 56, 57, 58,  1, 15]) then output should be 47\n",
      "if input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then output should be 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for i in range(len(x)):\n",
    "    print(f\"if input is {x[:i+1]} then output should be {y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str, batch_size: int):\n",
    "    data = train_data if split.lower() == \"train\" else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "Given tensor([24]) target is: 43\n",
      "Given tensor([24, 43]) target is: 58\n",
      "Given tensor([24, 43, 58]) target is: 5\n",
      "Given tensor([24, 43, 58,  5]) target is: 57\n",
      "Given tensor([24, 43, 58,  5, 57]) target is: 1\n",
      "Given tensor([24, 43, 58,  5, 57,  1]) target is: 46\n",
      "Given tensor([24, 43, 58,  5, 57,  1, 46]) target is: 43\n",
      "Given tensor([24, 43, 58,  5, 57,  1, 46, 43]) target is: 39\n",
      "Given tensor([44]) target is: 53\n",
      "Given tensor([44, 53]) target is: 56\n",
      "Given tensor([44, 53, 56]) target is: 1\n",
      "Given tensor([44, 53, 56,  1]) target is: 58\n",
      "Given tensor([44, 53, 56,  1, 58]) target is: 46\n",
      "Given tensor([44, 53, 56,  1, 58, 46]) target is: 39\n",
      "Given tensor([44, 53, 56,  1, 58, 46, 39]) target is: 58\n",
      "Given tensor([44, 53, 56,  1, 58, 46, 39, 58]) target is: 1\n",
      "Given tensor([52]) target is: 58\n",
      "Given tensor([52, 58]) target is: 1\n",
      "Given tensor([52, 58,  1]) target is: 58\n",
      "Given tensor([52, 58,  1, 58]) target is: 46\n",
      "Given tensor([52, 58,  1, 58, 46]) target is: 39\n",
      "Given tensor([52, 58,  1, 58, 46, 39]) target is: 58\n",
      "Given tensor([52, 58,  1, 58, 46, 39, 58]) target is: 1\n",
      "Given tensor([52, 58,  1, 58, 46, 39, 58,  1]) target is: 46\n",
      "Given tensor([25]) target is: 17\n",
      "Given tensor([25, 17]) target is: 27\n",
      "Given tensor([25, 17, 27]) target is: 10\n",
      "Given tensor([25, 17, 27, 10]) target is: 0\n",
      "Given tensor([25, 17, 27, 10,  0]) target is: 21\n",
      "Given tensor([25, 17, 27, 10,  0, 21]) target is: 1\n",
      "Given tensor([25, 17, 27, 10,  0, 21,  1]) target is: 54\n",
      "Given tensor([25, 17, 27, 10,  0, 21,  1, 54]) target is: 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(\"train\", batch_size=4)\n",
    "print(xb.shape, yb.shape)\n",
    "print(xb)\n",
    "print(yb)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        xi = xb[i, :j+1]\n",
    "        yo = yb[i, j]\n",
    "        print(f\"Given {xi} target is: {yo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(65)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 31, 23, 21, 41, 24, 32, 11, 13, 41, 17, 24, 25, 53, 32, 40, 60, 38,\n",
      "         60,  1, 15, 12, 52, 55,  7, 29, 17,  9,  9, 10, 15, 22, 55, 49, 27, 23,\n",
      "         20,  7, 55, 11, 10, 50, 39,  2, 53, 47, 63, 61, 49, 20, 48]])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHj\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "result = model.generate(idx, max_new_tokens=50)\n",
    "\n",
    "print(result)\n",
    "print(decode(result[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45680570602417\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for step in range(10000):\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "\n",
    "    logits, loss = model(xb, targets=yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UClwanovead her t t APAUESLANEd titrake! GLTathed s,\n",
      "RI lk.\n",
      "Anordenare pine thal wh,\n",
      "T:\n",
      "\n",
      "\n",
      "eve,\n",
      "\n",
      "\n",
      "Wheathas wouns Ply\n",
      "MI lif corndiver ve. he thoutasen at, ond tren: ibrmesanouse hat rait ge kfals.\n",
      "Ge Goowhe.\n",
      "Mears is ise henel t gherdf meve.\n",
      "OKIVEENG ol by t?\n",
      "ANNGr uenowhisstee be\n",
      "G bll ICleer sly f gs and tougr come te bue misein aip:\n",
      "MBRGllinfre e pe mfas ale Fr?\n",
      "CEro nd, dare is th we we wes?\n",
      "DUS:\n",
      "N HEdigen's y ple th mest r w be prm an sll ck y h ind, athoulkeavo An us ding bang aulcal ppe th\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "result = model.generate(idx, max_new_tokens=500)\n",
    "\n",
    "print(decode(result[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(f\"{a}\\n{b}\\n{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)\n",
    "xbow2 = wei @ x\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei.softmax(-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5150, 0.4850, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3342, 0.2973, 0.3686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2330, 0.1956, 0.2701, 0.3013, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2028, 0.2231, 0.1988, 0.1965, 0.1788, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1514, 0.1827, 0.1263, 0.1103, 0.1483, 0.2809, 0.0000, 0.0000],\n",
       "        [0.1438, 0.1328, 0.1517, 0.1577, 0.1505, 0.1194, 0.1442, 0.0000],\n",
       "        [0.1049, 0.1558, 0.0818, 0.0683, 0.0813, 0.2506, 0.1041, 0.1533]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "wei = wei * head_size**-0.5 # normalization so the wei values are evenly diffused instead of having too high of a value in one node which will make the softmax \"spiky\"(focused on a single node, instead of ~evenly across all)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei.softmax(-1)\n",
    "\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 8]),\n",
       " torch.Size([4, 8, 16]),\n",
       " torch.Size([4, 8, 16]),\n",
       " tensor([[ 0.0660,  0.0865, -0.0022, -0.0979,  0.0494, -0.0847, -0.1617, -0.0495,\n",
       "           0.1284,  0.1332,  0.0091,  0.0597,  0.1579, -0.0382,  0.0418, -0.0894],\n",
       "         [-0.5969,  0.1531, -0.4713, -0.3066,  0.6378,  0.4022, -0.3253,  0.2137,\n",
       "          -0.0749,  0.3800,  0.3823, -0.2537,  0.3899, -0.0850, -0.3499, -0.4370],\n",
       "         [ 0.3468,  0.2170,  0.1132, -0.2115, -0.0160, -0.3525, -0.3953, -0.2007,\n",
       "           0.3985,  0.2972, -0.0700,  0.2409,  0.3679, -0.0912,  0.2126, -0.1533],\n",
       "         [ 0.5402,  0.3231,  0.1842, -0.3105, -0.0421, -0.5434, -0.5874, -0.3089,\n",
       "           0.6036,  0.4377, -0.1165,  0.3708,  0.5441, -0.1352,  0.3307, -0.2190],\n",
       "         [ 0.7093, -0.2305,  0.5855,  0.4262, -0.8141, -0.4590,  0.4793, -0.2418,\n",
       "           0.0326, -0.5338, -0.4784,  0.2866, -0.5577,  0.1233,  0.4144,  0.5839],\n",
       "         [-0.9490, -0.3276, -0.4496,  0.2398,  0.3522,  0.8613,  0.5735,  0.4826,\n",
       "          -0.7818, -0.3626,  0.3240, -0.5780, -0.4896,  0.1273, -0.5737,  0.0653],\n",
       "         [ 0.0206,  0.1588, -0.0698, -0.1983,  0.1681, -0.0777, -0.3021, -0.0484,\n",
       "           0.1930,  0.2646,  0.0683,  0.0589,  0.3052, -0.0724,  0.0171, -0.2032],\n",
       "         [ 0.1876, -0.8071,  0.5463,  1.0629, -1.0799,  0.1686,  1.5517,  0.1226,\n",
       "          -0.8573, -1.4043, -0.4973, -0.1524, -1.5970,  0.3755,  0.0869,  1.1475]],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5599, 0.4401, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3220, 0.2016, 0.4764, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1640, 0.0815, 0.2961, 0.4585, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2051, 0.3007, 0.1894, 0.1808, 0.1241, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0600, 0.1273, 0.0291, 0.0169, 0.0552, 0.7114, 0.0000, 0.0000],\n",
       "         [0.1408, 0.1025, 0.1744, 0.2038, 0.1690, 0.0669, 0.1426, 0.0000],\n",
       "         [0.0223, 0.1086, 0.0082, 0.0040, 0.0080, 0.7257, 0.0216, 0.1016]],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x)\n",
    "xbow4 = wei @ v\n",
    "wei.shape, v.shape, xbow4.shape, v[0], wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril[:T, :T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT from scratch from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "ctx_len = 16\n",
    "batch_size = 4\n",
    "n_embed = 32\n",
    "head_size = 16\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data file\n",
    "with open(\"input.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# calculate vocab and vocab_size\n",
    "unique_chars = sorted(list(set(text)))\n",
    "vocab = unique_chars\n",
    "vocab_size = len(unique_chars)\n",
    "\n",
    "# make tokenizer with encode and decode functions\n",
    "stoi = {char: i for i, char in enumerate(vocab)}\n",
    "itos = {i: char for i, char in enumerate(vocab)}\n",
    "\n",
    "encode = lambda prompt: [stoi[char] for char in prompt]\n",
    "decode = lambda tokens: \"\".join([itos[i] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and split text into train and validation sets\n",
    "encoded_text = torch.tensor(encode(text))\n",
    "split_idx = int(0.9 * len(encoded_text))\n",
    "train_data = encoded_text[:split_idx]\n",
    "val_data = encoded_text[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540, True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data), len(train_data)+len(val_data) == len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data():\n",
    "    sample_idxs = torch.randint(len(train_data) - ctx_len, (batch_size,))\n",
    "    input_data = torch.stack([train_data[idx: idx+ctx_len] for idx in sample_idxs])\n",
    "    expected_data = torch.stack([train_data[idx+1: idx+ctx_len+1] for idx in sample_idxs])\n",
    "    \n",
    "    input_data.to(device)\n",
    "    expected_data.to(device)\n",
    "    \n",
    "    return input_data, expected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = batch_data()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(502859)\n",
      "[47, 57, 57, 1, 41, 53, 52, 57, 59, 51, 43, 10, 1, 58, 46, 43]\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tensor(645403)\n",
      "[53, 52, 42, 43, 56, 1, 47, 52, 1, 43, 62, 58, 56, 43, 51, 43]\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tensor(597081)\n",
      "[57, 1, 54, 47, 43, 56, 41, 47, 52, 45, 1, 58, 39, 50, 53, 52]\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tensor(148360)\n",
      "[43, 1, 56, 53, 59, 45, 46, 6, 1, 59, 52, 57, 61, 39, 63, 39]\n",
      "::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "for i in sample_idxs:\n",
    "    print(i)\n",
    "    print(train_data[i: i+ctx_len])\n",
    "    print(\":\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (int, tuple, device=str), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      5\u001b[0m n_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (int, tuple, device=str), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cpu'\n",
    "\n",
    "ctx_len = 4\n",
    "n_embed = 3\n",
    "\n",
    "torch.arange(ctx_len, , device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from scratch GPT-2 (tinkering section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_hf =GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M param model\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0261, 0.4674, 0.0604, 0.8971, 0.8544, 0.0498, 0.7623, 0.6229,\n",
      "          0.7601, 0.1649, 0.1788, 0.5903]]])\n",
      "tensor([[[0.0261, 0.4674, 0.0604, 0.8971]]]) tensor([[[0.8544, 0.0498, 0.7623, 0.6229]]]) tensor([[[0.7601, 0.1649, 0.1788, 0.5903]]])\n",
      "tensor([[-0.3119,  0.2621, -0.6726, -0.1256],\n",
      "        [ 0.0124,  0.3913, -0.4530, -0.0064],\n",
      "        [ 1.2756, -0.3220,  1.0417,  0.7482],\n",
      "        [-0.7600, -0.4858, -0.3814, -0.2147]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3119,    -inf,    -inf,    -inf],\n",
       "        [ 0.0124,  0.3913,    -inf,    -inf],\n",
       "        [ 1.2756, -0.3220,  1.0417,    -inf],\n",
       "        [-0.7600, -0.4858, -0.3814, -0.2147]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "st = torch.rand((1, 1, 12))\n",
    "print(st)\n",
    "q, k, v = st.split(4, dim=2)\n",
    "q: Tensor = q\n",
    "print(q, k, v)\n",
    "\n",
    "# q.view(q.size(0), q.size(1), 2, q.size(2) // 2)\n",
    "\n",
    "wei = torch.randn((4, 4))\n",
    "bias = torch.tril(torch.ones(4, 4))\n",
    "print(wei)\n",
    "print(bias)\n",
    "wei.masked_fill(bias == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 2\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 3\n",
    "    n_embd: int = 12\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "config = GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=12, out_features=36, bias=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 3, (1, config.block_size, config.n_embd), dtype=torch.float)\n",
    "print(x.shape)\n",
    "B, T, C = x.shape\n",
    "q, k, v  = c_attn(x).split(config.n_embd, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]) torch.Size([1, 2, 12]) torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3409, -1.2845,  0.6112,  0.9483,  1.0688, -0.0862, -0.3374,\n",
       "          -0.0135, -0.6908,  0.8617, -1.5078, -0.1678,  0.0754,  0.1991,\n",
       "          -0.0390, -0.9109, -0.2457,  1.2713, -0.0972,  0.1241, -0.4713,\n",
       "          -0.0112, -1.2633,  0.6746, -0.1899, -0.3594,  0.2570, -0.2694,\n",
       "          -0.2057, -0.4346,  0.6963, -1.2675, -0.3681,  0.7449,  0.5082,\n",
       "          -0.1041],\n",
       "         [ 1.3409, -1.2845,  0.6112,  0.9483,  1.0688, -0.0862, -0.3374,\n",
       "          -0.0135, -0.6908,  0.8617, -1.5078, -0.1678,  0.0754,  0.1991,\n",
       "          -0.0390, -0.9109, -0.2457,  1.2713, -0.0972,  0.1241, -0.4713,\n",
       "          -0.0112, -1.2633,  0.6746, -0.1899, -0.3594,  0.2570, -0.2694,\n",
       "          -0.2057, -0.4346,  0.6963, -1.2675, -0.3681,  0.7449,  0.5082,\n",
       "          -0.1041]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "x = torch.randint(0, 3, (1, config.block_size, config.n_embd), dtype=torch.float)\n",
    "x = torch.ones((1, config.block_size, config.n_embd), dtype=torch.float)\n",
    "print(x, x.shape, x.dtype)\n",
    "c_attn(x) # RuntimeError: mat1 and mat2 must have the same dtype, but got Long and Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7899,  0.1959,  0.4042, -0.3287, -1.1043, -0.1111,  0.2066,\n",
       "           -1.6848,  0.1115, -0.2385, -0.0914, -0.6865],\n",
       "          [-0.7899,  0.1959,  0.4042, -0.3287, -1.1043, -0.1111,  0.2066,\n",
       "           -1.6848,  0.1115, -0.2385, -0.0914, -0.6865]]],\n",
       "        grad_fn=<SplitBackward0>),\n",
       " tensor([[[ 0.2789, -0.5755,  0.5930, -0.3427,  0.5123,  0.0179,  0.2183,\n",
       "            0.5700,  0.1906,  0.2156, -0.8103,  0.0099],\n",
       "          [ 0.2789, -0.5755,  0.5930, -0.3427,  0.5123,  0.0179,  0.2183,\n",
       "            0.5700,  0.1906,  0.2156, -0.8103,  0.0099]]],\n",
       "        grad_fn=<SplitBackward0>),\n",
       " tensor([[[-0.1589, -0.5359, -0.5295, -0.2036, -0.0122, -0.4513,  0.1256,\n",
       "            0.1415,  0.0042, -0.6327,  0.8625,  0.3494],\n",
       "          [-0.1589, -0.5359, -0.5295, -0.2036, -0.0122, -0.4513,  0.1256,\n",
       "            0.1415,  0.0042, -0.6327,  0.8625,  0.3494]]],\n",
       "        grad_fn=<SplitBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_attn(x).split(config.n_embd, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 12]), torch.Size([1, 2, 12]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2, 4])\n",
      "torch.Size([1, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "q_v = q.view(B, T, config.n_head, C // config.n_head).transpose(1, 2) # torch.Size([1, 3, 2, 4])\n",
    "k_v = k.view(B, T, config.n_head, C // config.n_head).transpose(1, 2) # torch.Size([1, 3, 2, 4])\n",
    "print(q_v.shape)\n",
    "print(k_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 4])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(k_v.transpose(-2,-1).shape)\n",
    "wei = q_v @ k_v.transpose(-2,-1) # (B, n_head, T, head_size) @ (B, n_head, head_size , T) -> (B, n_head, T, T)\n",
    "\n",
    "out = wei @ q_v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.cat(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0.],\n",
       "          [1., 1.]]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_t = torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size)\n",
    "l_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../input.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "encoded_text = enc.encode(text)\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(encoded_text[:B*T+1])\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "encoded_text = enc.encode(text)\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(encoded_text[:B*T+1])\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderLite:\n",
    "    def __init__(self, B, T) -> None:\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open(\"../input.txt\", \"r\") as file:\n",
    "            text = file.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        self.tokens = torch.tensor(enc.encode(text))\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
    "\n",
    "        self.current_pos = 0\n",
    "\n",
    "    def get_batch(self):\n",
    "        curr_batch = self.tokens[self.current_pos:self.current_pos + (B*T) + 1]\n",
    "        x = curr_batch[:-1].view(B, T)\n",
    "        y = curr_batch[1:].view(B, T)\n",
    "\n",
    "        self.current_pos += B*T\n",
    "\n",
    "        if self.current_pos + (B*T) + 1 > len(self.tokens):\n",
    "            self.current_pos = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite():\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open(\"../input.txt\", \"r\") as file:\n",
    "            text = file.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        self.tokens = torch.tensor(enc.encode(text))\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"One epoch = {len(self.tokens) // (B*T)} batches\")\n",
    "        self.current_pos = 0\n",
    "    \n",
    "    def get_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_pos:self.current_pos + B*T + 1]\n",
    "        \n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "\n",
    "        self.current_pos += B*T\n",
    "        if self.current_pos + B*T + 1 > len(self.tokens):\n",
    "            self.current_pos = 0\n",
    "        \n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338025 tokens\n",
      "One epoch = 2640 batches\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
       "           3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
       "            461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
       "           1639,   389],\n",
       "         [  477, 12939,  2138,   284,  4656,   621,   284,  1145,   680,    30,\n",
       "            198,   198,  3237,    25,   198,  4965,  5634,    13, 12939,    13,\n",
       "            198,   198,  5962, 22307,    25,   198,  5962,    11,   345,   760,\n",
       "            327,  1872],\n",
       "         [  385,  1526, 28599,   318,  4039,  4472,   284,   262,   661,    13,\n",
       "            198,   198,  3237,    25,   198,  1135,   760,   470,    11,   356,\n",
       "            760,   470,    13,   198,   198,  5962, 22307,    25,   198,  5756,\n",
       "            514,  1494],\n",
       "         [  683,    11,   290,   356,  1183,   423, 11676,   379,   674,   898,\n",
       "           2756,    13,   198,  3792,   470,   257, 15593,    30,   198,   198,\n",
       "           3237,    25,   198,  2949,   517,  3375,   319,   470,    26,  1309,\n",
       "            340,   307]]),\n",
       " tensor([[22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,  3285,\n",
       "            502,  2740,    13,   198,   198,  3237,    25,   198,  5248,   461,\n",
       "             11,  2740,    13,   198,   198,  5962, 22307,    25,   198,  1639,\n",
       "            389,   477],\n",
       "         [12939,  2138,   284,  4656,   621,   284,  1145,   680,    30,   198,\n",
       "            198,  3237,    25,   198,  4965,  5634,    13, 12939,    13,   198,\n",
       "            198,  5962, 22307,    25,   198,  5962,    11,   345,   760,   327,\n",
       "           1872,   385],\n",
       "         [ 1526, 28599,   318,  4039,  4472,   284,   262,   661,    13,   198,\n",
       "            198,  3237,    25,   198,  1135,   760,   470,    11,   356,   760,\n",
       "            470,    13,   198,   198,  5962, 22307,    25,   198,  5756,   514,\n",
       "           1494,   683],\n",
       "         [   11,   290,   356,  1183,   423, 11676,   379,   674,   898,  2756,\n",
       "             13,   198,  3792,   470,   257, 15593,    30,   198,   198,  3237,\n",
       "             25,   198,  2949,   517,  3375,   319,   470,    26,  1309,   340,\n",
       "            307,  1760]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoaderLite(4, 32)\n",
    "\n",
    "dl.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.0430)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(12)\n",
    "n = 100\n",
    "for i in range(n):\n",
    "    resid_amt = torch.randn(12)\n",
    "    scaled_resid_amt = resid_amt * n**-0.5\n",
    "    x += resid_amt\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9746318461970762\n",
      "Adjacency Matrix:\n",
      " [[0.         0.97463185 0.95941195]\n",
      " [0.97463185 0.         0.99819089]\n",
      " [0.95941195 0.99819089 0.        ]]\n",
      "Left Singular Vectors (Embeddings):\n",
      " [[-0.57322641  0.16471728 -0.80267036]\n",
      " [-0.58087648 -0.77259253  0.25628752]\n",
      " [-0.57792214  0.6131631   0.53855084]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# Step 1: Cosine Similarity\n",
    "vector_a = np.array([1, 2, 3])\n",
    "vector_b = np.array([4, 5, 6])\n",
    "cosine_similarity = np.dot(vector_a, vector_b) / (np.linalg.norm(vector_a) * np.linalg.norm(vector_b))\n",
    "print(\"Cosine Similarity:\", cosine_similarity)\n",
    "\n",
    "# Step 2: Creating an Adjacency Matrix\n",
    "embeddings = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "def cosine_similarity_matrix(embeddings):\n",
    "    num_vectors = embeddings.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_vectors, num_vectors))\n",
    "    for i in range(num_vectors):\n",
    "        for j in range(num_vectors):\n",
    "            if i != j:\n",
    "                adjacency_matrix[i, j] = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "    return adjacency_matrix\n",
    "\n",
    "adj_matrix = cosine_similarity_matrix(embeddings)\n",
    "print(\"Adjacency Matrix:\\n\", adj_matrix)\n",
    "\n",
    "# Step 3: Singular Value Decomposition (SVD)\n",
    "U, S, Vt = svd(adj_matrix)\n",
    "print(\"Left Singular Vectors (Embeddings):\\n\", U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploring other archs to see the internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.decoder.weight torch.Size([30522, 768])\n",
      "cls.predictions.decoder.bias torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "\n",
    "model_hf= AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model_hf_2 = AutoModel.from_pretrained('google-bert/bert-base-uncased')\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Number of tokens to predict\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Predict the next n tokens\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n)):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Mask the next token\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Number of tokens to predict\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Predict the next n tokens\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n)):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Mask the next token\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Repos/NNs-from-scratch/.venv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/NNs-from-scratch/.venv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the input text\n",
    "input_text = [\n",
    "    \"we are gathered here today to \",\n",
    "    \"we are gathered here today to \"\n",
    "]\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Number of tokens to predict\n",
    "n = 10\n",
    "\n",
    "# Predict the next n tokens\n",
    "for _ in tqdm(range(n)):\n",
    "    # Mask the next token\n",
    "    input_ids = torch.cat([input_ids, torch.tensor([[tokenizer.mask_token_id]])], dim=1) # torch.Size([1, 8])\n",
    "\n",
    "    # Get the predictions for the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs.logits # torch.Size([1, 8, 30522])\n",
    "\n",
    "    idx = predictions[:, -1, :] # torch.Size([1, 30522])\n",
    "    probs = F.softmax(idx, dim=-1)\n",
    "\n",
    "    topk_probs, topk_idxs = torch.topk(probs, k=25, dim=-1)\n",
    "    sampled_tok = torch.multinomial(topk_probs, num_samples=1)\n",
    "    new_tok = torch.gather(topk_idxs, -1, sampled_tok)\n",
    "\n",
    "    # Append the predicted token to the input IDs\n",
    "    input_ids[0, -1] = new_tok\n",
    "\n",
    "# Decode the final input IDs to get the output text\n",
    "input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"Generated text: {input_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2057, 2024, 5935, 2182, 2651, 2000, 8439, 1996, 2166, 1997,  102],\n",
      "        [ 101, 2057, 2024, 5935, 2182, 2651, 2000, 8439, 1996, 2166, 1997,  102],\n",
      "        [ 101, 2057, 2024, 5935, 2182, 2651, 2000, 8439, 1996, 2166, 1997,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['we are gathered here today to celebrate the life of',\n",
       " 'we are gathered here today to celebrate the life of',\n",
       " 'we are gathered here today to celebrate the life of']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input text\n",
    "input_text = [\n",
    "    \"we are gathered here today to celebrate the life of \",\n",
    "    \"we are gathered here today to celebrate the life of \",\n",
    "    \"we are gathered here today to celebrate the life of \",\n",
    "]\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors='pt')\n",
    "print(input_ids)\n",
    "tokenizer.batch_decode(input_ids['input_ids'], skip_special_tokens=True) # TypeError: int() argument must be a string, a bytes-like object or a real number, not 'list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tok_id = torch.argmax(probs, dim=1)\n",
    "predicted_tok_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(predicted_tok_id.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape, predictions.shape, idx.shape, probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2057, 2024, 5935, 2182, 2651, 2000, 8439, 1996, 2166, 1997,  102],\n",
       "        [ 101, 2057, 2024, 5935, 2182, 2651, 2000, 8439, 1996, 2166, 1997,  102],\n",
       "        [ 101, 2057, 2024, 5935, 2182, 2651, 2000, 8439, 1996, 2166, 1997,  102]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = input_ids['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[103],\n",
       "         [103],\n",
       "         [103]]),\n",
       " tensor([103, 103, 103]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((input_ids.size(0), 1), 103), torch.full((input_ids.size(0), 1), 103).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2057, 2024, 5935, 2182, 2651, 2000, 8439, 1996, 2166, 1997, 102]\n",
      "[101, 2057, 2024, 5935, 2182, 2651, 2000, 102]\n",
      "[101, 2057, 2024, 5935, 2182, 2651, 2000, 2425, 2026, 2564, 2008, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([ 101, 2057, 2024, 5935, 2182, 2651, 2000, 8439, 1996, 2166, 1997,  102,\n",
       "          103]),\n",
       " tensor([ 101, 2057, 2024, 5935, 2182, 2651, 2000,  102,  103]),\n",
       " tensor([ 101, 2057, 2024, 5935, 2182, 2651, 2000, 2425, 2026, 2564, 2008,  102,\n",
       "          103])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input_ids = []\n",
    "for sample in input_ids:\n",
    "    print(sample)\n",
    "    masked_input_ids.append(torch.cat([torch.tensor(sample), torch.tensor([tokenizer.mask_token_id])], dim=-1))\n",
    "masked_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_token_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "torch.tensor(input_ids.size(0), tokenizer.mask_token_id).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: the capital of france is paris\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted token ID for the last masked token\n",
    "masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1][-1]\n",
    "predicted_token_id = torch.argmax(predictions[:, masked_index, :], dim=1)\n",
    "\n",
    "# Append the predicted token to the input IDs\n",
    "input_ids[0, masked_index] = predicted_token_id\n",
    "\n",
    "# Decode the final input IDs to get the output text\n",
    "output_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"Generated text: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in = \"i love dogs\"\n",
    "\n",
    "maked_in = \"i [MASK] dogs\"\n",
    "\n",
    "in = \"i love dogs [MASK]\"\n",
    "out = \"i love dogs very\"\n",
    "in_2 = \"i love dogs very [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9999999999999997e-05\n",
      "5.9999999999999995e-05\n",
      "8.999999999999999e-05\n",
      "0.00011999999999999999\n",
      "0.00015\n",
      "0.00017999999999999998\n",
      "0.00020999999999999998\n",
      "0.00023999999999999998\n",
      "0.00026999999999999995\n",
      "0.0003\n",
      "0.0002999999999999999\n",
      "0.0002995838400539722\n",
      "0.0002983379259803436\n",
      "0.00029626993925368627\n",
      "0.0002933926296998457\n",
      "0.00028972373688902363\n",
      "0.0002852858807654296\n",
      "0.0002801064221878024\n",
      "0.00027421729424061787\n",
      "0.00026765480535600414\n",
      "0.00026045941546018383\n",
      "0.0002526754865245747\n",
      "0.0002443510090594838\n",
      "0.00023553730623665305\n",
      "0.00022628871746483876\n",
      "0.00021666226336928708\n",
      "0.00020671729424061788\n",
      "0.00019651512412054723\n",
      "0.00018611865278043115\n",
      "0.00017559197792325906\n",
      "0.00016499999999999997\n",
      "0.0001544080220767409\n",
      "0.0001438813472195688\n",
      "0.0001334848758794528\n",
      "0.0001232827057593821\n",
      "0.00011333773663071286\n",
      "0.00010371128253516117\n",
      "9.446269376334689e-05\n",
      "8.564899094051613e-05\n",
      "7.732451347542522e-05\n",
      "6.954058453981609e-05\n",
      "6.234519464399581e-05\n",
      "5.5782705759382104e-05\n",
      "4.989357781219755e-05\n",
      "4.471411923457034e-05\n",
      "4.027626311097629e-05\n",
      "3.6607370300154266e-05\n",
      "3.3730060746313664e-05\n",
      "3.1662074019656414e-05\n",
      "3.0416159946027724e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "max_lr = 3e-4\n",
    "min_lr = 3e-4 * 0.1\n",
    "warmup_steps = 10\n",
    "total_steps = 50\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step+1) / warmup_steps\n",
    "    if step > total_steps:\n",
    "        return min_lr\n",
    "    # cosine decay\n",
    "    decay_ratio = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)      \n",
    "\n",
    "for i in range(total_steps):\n",
    "    print(get_lr(i))\n",
    "\n",
    "lrs = [get_lr(i) for i in range(total_steps)]\n",
    "\n",
    "plt.plot(range(total_steps), lrs)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt2_from_scratch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mnamed_parameters()\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    ".from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "\n",
    "def create_mlm_inputs(inputs, tokenizer, mlm_probability=0.15):\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.42.4\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "config = BertConfig.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "bert_tok = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "type(bert_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 1049, 1037, 2653, 2944, 1010, 102], [101, 1045, 1005, 1049, 1037, 2653, 2944, 1010, 102], [101, 1045, 1005, 1049, 1037, 2653, 2944, 1010, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    ['Hi there world', 'arent you special'],\n",
    "    ['Hi there world', 'arent you special'],\n",
    "    ['Hi there world', 'arent you special']\n",
    "]\n",
    "docs = [\n",
    "    \"I'm a language model, \",\n",
    "    \"I'm a language model, \",\n",
    "    \"I'm a language model, \"\n",
    "]\n",
    "\n",
    "inputs = bert_tok(docs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,  7592,  1010,\n",
       "          2026,  4937,  2003, 10140,  2205,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = bert_tok(\"Hello, my dog is cute\", \"Hello, my cat is cute too\", padding=True, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2851134/4038664820.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_area pre {white-space: pre;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 1045, 2572, 1037, 3407, 3062, 2050, 2007, 1037, 3407, 2166,  102])\n",
      "tensor([ True,  True,  True, False,  True,  True,  True, False,  True,  True,\n",
      "        False,  True])\n",
      "tensor([ 103,  103,  103, 1037,  103,  103,  103, 2007,  103,  103, 2166,  103])\n",
      "rand_tok_idx: tensor([16958,  4983, 28259, 25823, 20347, 13050, 16766, 18712, 10733,    25,\n",
      "        21839, 17663])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([16958,  4983, 28259,  1037, 20347, 13050, 16766,  2007, 10733,    25,\n",
       "         2166, 17663])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_in = torch.tensor(bert_tok.encode(\"I am a happy fella with a happy life\"))\n",
    "idx_to_mask = torch.bernoulli(torch.full(ex_in.size(), 0.8)).bool()\n",
    "print(ex_in)\n",
    "print(idx_to_mask)\n",
    "ex_in[idx_to_mask] = bert_tok.mask_token_id\n",
    "print(ex_in)\n",
    "\n",
    "random_tok_indices = torch.randint(len(tokenizer), ex_in.size(), dtype=torch.long)\n",
    "print(f\"rand_tok_idx: {random_tok_indices}\")\n",
    "ex_in[idx_to_mask] = random_tok_indices[idx_to_mask]\n",
    "ex_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def create_mlm_inputs_prac(inputs: Tensor, tokenizer: BertTokenizer, mlm_probability: float = 0.15):\n",
    "    # copy the inputs to be assigned as ground_truth lables to calculate loss against\n",
    "    labels = inputs.clone()\n",
    "\n",
    "    # pick mlm_probability(15% default) of the tokens to be maksed from the entire inputs set of tokens\n",
    "    probability_matrix = torch.full(inputs.size(), mlm_probability)\n",
    "    # `masked_indicies` holds which tokens can be masked\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    # We only compute loss on masked tokens, so set any unmasked tokens in `labels` to -100 which is what pytorch expects if you dont want a token involved in loss calculation\n",
    "    labels[~masked_indices] = -100\n",
    "\n",
    "    # of the tokens to be masked(masked_indices) select 80% to be masked with the [MASK] token\n",
    "    mask_token_indices = torch.bernoulli(torch.full(inputs.size(), 0.8)).bool() & masked_indices\n",
    "    inputs[mask_token_indices] = tokenizer.mask_token_id\n",
    "\n",
    "    # of the tokens to be masked(masked_indices),\n",
    "    # but not the once that already have been replaced by [MASK],\n",
    "    # pick 50% of them to be replaced with a random token from the tokenizer\n",
    "    rand_token_mask_indices = torch.bernoulli(torch.full(inputs.size(), 0.5)).bool() & masked_indices & ~mask_token_indices\n",
    "    random_tok_indices = torch.randint(len(tokenizer), labels.size(), dtype=torch.long)\n",
    "    inputs[rand_token_mask_indices] = random_tok_indices[rand_token_mask_indices]\n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Inputs: tensor([[  101,  7592,  1010,  2026,  3899,   103, 10140,   103,   103,  1045,\n",
      "           103,  1037,   103,  3608,  2007,  2061,  2116,  2576, 17396,   102]])\n",
      "MLM Inputs 1: tensor([[  101,  7592,  1010,  2026,  3899,   103, 10140,   103,   103,  1045,\n",
      "           103,  1037,   103,  3608,  2007,  2061,  2116,  2576, 17396,   102]])\n",
      "MLM Inputs 2: tensor([[  101,  7592,  1010,  2026,  3899,   103, 10140,   103,   103,  1045,\n",
      "           103,  1037,   103,  3608,  2007,  2061,  2116,  2576, 17396,   102]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True]])\n",
      "Labels 1: tensor([[ 101, -100, -100, -100, -100, 2003, -100, -100, -100, -100, -100, -100,\n",
      "         2502, -100, -100, -100, -100, -100, -100, -100]])\n",
      "Labels 2: tensor([[-100, -100, -100, -100, -100, -100, -100, 1010, 1998, -100, 2031, -100,\n",
      "         -100, -100, -100, -100, -100, -100, 4784, -100]])\n",
      "tensor([[False,  True,  True,  True,  True, False,  True, False, False,  True,\n",
      "         False,  True, False,  True,  True,  True,  True,  True, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input (tokenized sentences)\n",
    "inputs = torch.tensor([tokenizer.encode(\"Hello, my dog is cute, and I have a big ball with so many political ideas\")])\n",
    "\n",
    "def create_mlm_inputs(inputs, tokenizer, mlm_probability=0.15):\n",
    "    labels = inputs.clone()\n",
    "\n",
    "    # pick mlm_probability(15% default) of the tokens to be maksed from the entire inputs set of tokens\n",
    "    # masked_indicies holds which tokens can be masked\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # of the tokens to be masked(masked_indices) select 80% to be masked with the [MASK] token\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # of the tokens to be masked(masked_indices),\n",
    "    # but not the once that already have been replaced by [MASK],\n",
    "    # pick 50% of them to be replaced with a random token from the tokenizer\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "# Create MLM inputs\n",
    "inputs_mlm, labels_mlm = create_mlm_inputs(inputs, tokenizer)\n",
    "inputs_mlm_2, labels_mlm_2 = create_mlm_inputs_prac(inputs, tokenizer)\n",
    "\n",
    "print(\"Original Inputs:\", inputs)\n",
    "print(\"MLM Inputs 1:\", inputs_mlm)\n",
    "print(\"MLM Inputs 2:\", inputs_mlm_2)\n",
    "print(inputs_mlm == inputs_mlm_2)\n",
    "print(\"Labels 1:\", labels_mlm)\n",
    "print(\"Labels 2:\", labels_mlm_2)\n",
    "print(labels_mlm == labels_mlm_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mlm_probability = 0.15\n",
    "\n",
    "inputs = torch.tensor(bert_tok.encode(\"I am a happy fella with a happy life\"))\n",
    "labels = inputs.clone()\n",
    "inputs, labels\n",
    "\n",
    "torch.bernoulli(torch.full(inputs.size(), mlm_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
      "tensor([False, False, False, False, False,  True,  True, False,  True, False,\n",
      "        False,  True])\n"
     ]
    }
   ],
   "source": [
    "probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "masked_indices = torch.bernoulli(probability_matrix)\n",
    "print(masked_indices)\n",
    "masked_indices = masked_indices.bool()\n",
    "print(masked_indices)\n",
    "# masked_pct = []\n",
    "# for i in range(10):\n",
    "#     masked_indices = torch.bernoulli(probability_matrix)\n",
    "#     pct_masked = masked_indices.count_nonzero() / len(masked_indices)\n",
    "#     masked_pct.append(pct_masked)\n",
    "# print(sum(masked_pct)/len(masked_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 1045, 2572, 1037, 3407, 3062, 2050, 2007, 1037, 3407, 2166,  102])\n",
      "tensor([-100, -100, -100, -100, -100, 3062, 2050, -100, 1037, -100, -100,  102])\n",
      "masked_indicies: tensor([False, False, False, False, False,  True,  True, False,  True, False,\n",
      "        False,  True])\n",
      "new_mask: tensor([ True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True])\n",
      "combined_mask: tensor([False, False, False, False, False,  True,  True, False,  True, False,\n",
      "        False,  True])\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "labels[~masked_indices] = -100\n",
    "print(labels)\n",
    "print(f\"masked_indicies: {masked_indices}\")\n",
    "print(f\"new_mask: {torch.bernoulli(torch.full(labels.shape, 0.8)).bool()}\")\n",
    "print(f\"combined_mask: {torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False,  True,  True, False,  True, False,\n",
      "        False,  True])\n",
      "pre_mask: tensor([ 101, 1045, 2572, 1037, 3407,  103,  103, 2007,  103, 3407, 2166,  103])\n",
      "post_mask: tensor([ 101, 1045, 2572, 1037, 3407,  103,  103, 2007,  103, 3407, 2166,  103])\n"
     ]
    }
   ],
   "source": [
    "indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "print(indices_replaced)\n",
    "print(f\"pre_mask: {inputs}\")\n",
    "inputs[indices_replaced] = bert_tok.convert_tokens_to_ids(bert_tok.mask_token)\n",
    "print(f\"post_mask: {inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "query = torch.randn(2,3,4)\n",
    "key = torch.randn(2,3,4)\n",
    "value= torch.randn(2,3,4)\n",
    "\n",
    "head_size = key.size(-1)\n",
    "\n",
    "wei_einsum = torch.einsum('ijk, ilk -> ijl', [query, key]) / math.sqrt(head_size)\n",
    "wei = (query @ key.transpose(-2, -1)) / math.sqrt(head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei_einsum.allclose(wei), wei.allclose(wei_einsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = wei @ value\n",
    "out_einsum = torch.einsum('ijk, ikl -> ijl', [wei, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.allclose(out_einsum), out_einsum.allclose(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# (B, T, n_heads, head_size)\n",
    "c_attn = torch.randn(2, 3, 8)\n",
    "query = torch.randn(2,3,2,4)\n",
    "key = torch.randn(2,3,2,4)\n",
    "value= torch.randn(2,3,2,4)\n",
    "\n",
    "head_size = key.size(-1)\n",
    "\n",
    "q_mod = query.transpose(1, 2)\n",
    "k_mod = key.transpose(1, 2)\n",
    "v_mod = value.transpose(1, 2)\n",
    "wei = (q_mod @ k_mod.transpose(-2, -1)) / math.sqrt(head_size)\n",
    "\n",
    "wei_einsum = torch.einsum('btqh, buqh -> bqtu', [query, key]) / math.sqrt(head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei_einsum.allclose(wei), wei.allclose(wei_einsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): invalid subscript given at index 2 in the equation string, subscripts must be in [a-zA-Z]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbt(qh) -> btqh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_attn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/NNs-from-scratch/.venv/lib/python3.11/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): invalid subscript given at index 2 in the equation string, subscripts must be in [a-zA-Z]"
     ]
    }
   ],
   "source": [
    "torch.einsum('bt(qh) -> btqh', c_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = wei @ v_mod\n",
    "out = out.transpose(1, 2).contiguous().view(2, 3, 8) # (B, n_head, T, head_size) -> (B, T, n_head, head_size) -> (B, T, C)\n",
    "out_einsum = torch.einsum('bqtT, bTqh -> btqh', [wei, value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_einsum.allclose(out), out.allclose(out_einsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6885, -1.0200,  0.3532],\n",
       "         [-1.5162, -0.2009,  1.7239],\n",
       "         [ 0.1174,  2.5692, -0.2188]]),\n",
       " torch.Size([3, 3]),\n",
       " tensor([[ 0.4756,  0.0774],\n",
       "         [-2.1972,  1.5348],\n",
       "         [ 0.0515,  1.9464]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x.shape, y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum('ij,kl->ijkl', [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.randn(2, 3, 8)\n",
    "B, T, C = x.shape\n",
    "\n",
    "q_proj = torch.nn.Linear(8, 8)\n",
    "k_proj = torch.nn.Linear(8, 8)\n",
    "v_proj = torch.nn.Linear(8, 8)\n",
    "\n",
    "n_heads = 4\n",
    "n_kv_heads = 2\n",
    "\n",
    "proj_q = q_proj(x)\n",
    "proj_k = k_proj(x)\n",
    "proj_v = v_proj(x)\n",
    "\n",
    "# Project x to query, key, and value\n",
    "query = proj_q.view(B, T, n_heads, C // n_heads)\n",
    "key = proj_k.view(B, T, n_kv_heads, C // n_kv_heads)\n",
    "value = proj_v.view(B, T, n_kv_heads, C // n_kv_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4, 2]), torch.Size([2, 3, 2, 4]), torch.Size([2, 3, 2, 4]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1100,  0.3767,  0.3367,  0.4931,  0.7002,  0.1908, -0.1777,\n",
       "           0.0034],\n",
       "         [ 0.3238,  0.4601,  0.2862, -0.0037,  1.7198,  0.0105,  0.9477,\n",
       "           0.0458],\n",
       "         [ 0.7135, -0.5920,  0.0598,  0.0561, -0.1415,  0.1503,  0.2890,\n",
       "          -0.0088]],\n",
       "\n",
       "        [[ 0.7336,  0.1392,  0.1895, -0.0306,  0.7440,  0.8309, -0.0636,\n",
       "          -0.6689],\n",
       "         [-0.7971,  0.2610,  1.1168,  0.0168,  0.9502,  0.0932,  0.8150,\n",
       "          -0.3616],\n",
       "         [ 0.9856,  0.4679,  0.3215, -0.3155,  0.2513, -0.0775, -0.6535,\n",
       "           0.3521]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1100,  0.3767],\n",
       "          [ 0.3238,  0.4601],\n",
       "          [ 0.7135, -0.5920]],\n",
       "\n",
       "         [[ 0.3367,  0.4931],\n",
       "          [ 0.2862, -0.0037],\n",
       "          [ 0.0598,  0.0561]],\n",
       "\n",
       "         [[ 0.7002,  0.1908],\n",
       "          [ 1.7198,  0.0105],\n",
       "          [-0.1415,  0.1503]],\n",
       "\n",
       "         [[-0.1777,  0.0034],\n",
       "          [ 0.9477,  0.0458],\n",
       "          [ 0.2890, -0.0088]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7336,  0.1392],\n",
       "          [-0.7971,  0.2610],\n",
       "          [ 0.9856,  0.4679]],\n",
       "\n",
       "         [[ 0.1895, -0.0306],\n",
       "          [ 1.1168,  0.0168],\n",
       "          [ 0.3215, -0.3155]],\n",
       "\n",
       "         [[ 0.7440,  0.8309],\n",
       "          [ 0.9502,  0.0932],\n",
       "          [ 0.2513, -0.0775]],\n",
       "\n",
       "         [[-0.0636, -0.6689],\n",
       "          [ 0.8150, -0.3616],\n",
       "          [-0.6535,  0.3521]]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 0.1100,  0.3767],\n",
       "           [ 0.3238,  0.4601],\n",
       "           [ 0.7135, -0.5920]],\n",
       "\n",
       "          [[ 0.3367,  0.4931],\n",
       "           [ 0.2862, -0.0037],\n",
       "           [ 0.0598,  0.0561]]],\n",
       "\n",
       "\n",
       "         [[[ 0.7002,  0.1908],\n",
       "           [ 1.7198,  0.0105],\n",
       "           [-0.1415,  0.1503]],\n",
       "\n",
       "          [[-0.1777,  0.0034],\n",
       "           [ 0.9477,  0.0458],\n",
       "           [ 0.2890, -0.0088]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.7336,  0.1392],\n",
       "           [-0.7971,  0.2610],\n",
       "           [ 0.9856,  0.4679]],\n",
       "\n",
       "          [[ 0.1895, -0.0306],\n",
       "           [ 1.1168,  0.0168],\n",
       "           [ 0.3215, -0.3155]]],\n",
       "\n",
       "\n",
       "         [[[ 0.7440,  0.8309],\n",
       "           [ 0.9502,  0.0932],\n",
       "           [ 0.2513, -0.0775]],\n",
       "\n",
       "          [[-0.0636, -0.6689],\n",
       "           [ 0.8150, -0.3616],\n",
       "           [-0.6535,  0.3521]]]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape query\n",
    "query = query.view(B, n_heads // n_kv_heads, n_kv_heads, T, C // n_heads)\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2, 3, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1100,  0.3767],\n",
       "         [ 0.3238,  0.4601],\n",
       "         [ 0.7135, -0.5920]],\n",
       "\n",
       "        [[ 0.3367,  0.4931],\n",
       "         [ 0.2862, -0.0037],\n",
       "         [ 0.0598,  0.0561]],\n",
       "\n",
       "        [[ 0.7002,  0.1908],\n",
       "         [ 1.7198,  0.0105],\n",
       "         [-0.1415,  0.1503]],\n",
       "\n",
       "        [[-0.1777,  0.0034],\n",
       "         [ 0.9477,  0.0458],\n",
       "         [ 0.2890, -0.0088]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.view(B, (n_heads // n_kv_heads) * n_kv_heads, T, C // n_heads)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1100,  0.3767],\n",
       "         [ 0.3367,  0.4931],\n",
       "         [ 0.7002,  0.1908],\n",
       "         [-0.1777,  0.0034]],\n",
       "\n",
       "        [[ 0.3238,  0.4601],\n",
       "         [ 0.2862, -0.0037],\n",
       "         [ 1.7198,  0.0105],\n",
       "         [ 0.9477,  0.0458]],\n",
       "\n",
       "        [[ 0.7135, -0.5920],\n",
       "         [ 0.0598,  0.0561],\n",
       "         [-0.1415,  0.1503],\n",
       "         [ 0.2890, -0.0088]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.view(B, (n_heads // n_kv_heads) * n_kv_heads, T, C // n_heads).transpose(1, 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.view(B, (n_heads // n_kv_heads) * n_kv_heads, T, C // n_heads).transpose(1, 2).view(B, T, (n_heads // n_kv_heads) * n_kv_heads * (C // n_heads), )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape query\n",
    "query = query.view(B, n_heads // n_kv_heads, n_kv_heads, T, C // n_heads)\n",
    "\n",
    "# Use einsum for the attention computation\n",
    "wei = torch.einsum('bghqd,bhkd->bghqk', query, key) / math.sqrt(n_heads)\n",
    "\n",
    "# Apply mask and softmax\n",
    "wei = wei.masked_fill(tril[:, :, :, :T, :T] == 0, float('-inf'))\n",
    "wei = wei.softmax(dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
